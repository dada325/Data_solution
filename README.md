# Data Solution Code
Find question to practise with.
Find practical solutions to your coding problems in data science.


## About this Page




### What will I build in this page?





## Description
When I first confront a problem of data science, I want to try to solve it with the knowledge from the book. But most of time it fails, the problem may be too complex and not easy to abstract to a computer problem. 

We understand the challenges and complexities that data scientists face while working on various projects. Our mission is to provide a comprehensive platform where data science enthusiasts and professionals can find practical solutions to their coding problems.

We want to cover a wide range of topics, including data preprocessing, data visualization, machine learning algorithms, statistical analysis, deep learning, natural language processing, Reinforcement learning and more. We believe in hands-on learning, so you'll find plenty of interactive coding exercises and projects that allow you to apply your newfound knowledge. Now the Contents is still Work in progress.

## Status:
 now WIP is the section of Reinforcement learning, so please feel free to fork of other section. 

## Machine Learning 

- Supervised Learning

- Unsupervised Learning

- Deep Learning

- Natural Language Processing (NLP)

- Computer Vision

- Recommender Systems

- Time Series Forecasting

- Transfer Learning

- Adversarial Machine Learning

- AutoML (Automated Machine Learning)

- Reinforcement Learning.


 0. The Basic

 ###Some of the library
| Library Name | Description |
| --- | --- |
| [OpenAI Baselines](https://github.com/openai/baselines) | OpenAI Baselines offers implementations of various RL algorithms. It supports training the model and also supports a logger to help visualize the training metrics. |
| [Stable Baselines](https://github.com/DLR-RM/stable-baselines3) | A fork of OpenAI Baseline library with improvements. It offers additional algorithms Soft Actor-Critic (SAC) and Twin Delayed DDPG (TD3) and supports Tensorboard. |
| [TF Agents](https://github.com/tensorflow/agents) | A Tensorflow library for reinforcement learning that provides various RL components that can be easily used or modified as per needs. |
| [Keras-RL](https://github.com/keras-rl/keras-rl) | A deep reinforcement learning library for Keras that has implementations of state-of-art RL algorithms. |
| [Keras-RL2](https://github.com/wau/keras-rl2) | A fork of the Keras-RL library that supports Tensorflow 2 in the original Keras-RL library. |
| [PyQlearning](https://github.com/chimera0/accel-brain-code/tree/master/Reinforcement-Learning) | A reinforcement learning library that focuses on Q Learning. It supports both deep Q learning and multi-agent deep Q learning. |
| [Tensorforce](https://github.com/tensorforce/tensorforce) | An open-source deep reinforcement learning library built on top of the Tensorflow library. |
| [RL Coach](https://github.com/NervanaSystems/coach) | Reinforcement Learning Coach is a reinforcement learning library created by Intel AI Lab to provide implementations of various state-of-art RL algorithms. |
| [Chainer RL](https://github.com/chainer/chainerrl) | A reinforcement library built on the deep learning framework Chainer to implement various state-of-art RL algorithms. |
| [Mushroom RL](https://github.com/MushroomRL/mushroom-rl) | A Python library for reinforcement learning that is simple yet powerful to run various RL algorithms. |
| [Acme](https://github.com/deepmind/acme) | A reinforcement learning framework created by Deepmind targeted more towards researchers to help in rapid prototyping and develop novel RL algorithms. |
| [Dopamine](https://github.com/google/dopamine) | A Google reinforcement learning framework to help researchers with the fast prototyping of RL algorithms. |
| [RLlib](https://github.com/ray-project/ray) | A reinforcement learning library that provides high scalability and a unified API for a variety of RL applications. |
| [TRFL](https://github.com/deepmind/trfl) | A Tensorflow based reinforcement learning framework that offers various building blocks for writing RL algorithms. |
| [ReAgent](https://github.com/facebookresearch/ReAgent) | A Facebook framework for reinforcement learning which offers various RL algorithms and supports data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, and optimized serving. |
| [MAgent](https://github.com/PettingZoo-Team/MAgent) | Supports multiple agents and can let you scale from hundreds to millions of agents. |
| [SLM Lab](https://github.com/slm-lab/slm-lab) | A reinforcement learning framework that offers modular components for flexible experimentations and reproducibility. |
| [DeeR](https://github.com/VictorPierre/DeeR) |


 1. The Intersection of Reinforcement Learning and other discipline
      * Economics
      * Psychology
      * Neuroscience
      * Computer Science
      * Engineering
      * Mathematic
 
 2. Some of the Paper reviews


| Algorithm                                       | Authors           | Year |
|-------------------------------------------------|-------------------|------|
| [A2C / A3C (Asynchronous Advantage Actor-Critic)](https://arxiv.org/abs/1602.01783) | Mnih et al        | 2016 |
| [PPO (Proximal Policy Optimization)](https://arxiv.org/abs/1707.06347)              | Schulman et al    | 2017 |
| [TRPO (Trust Region Policy Optimization)](https://arxiv.org/abs/1502.05477)         | Schulman et al    | 2015 |
| [DDPG (Deep Deterministic Policy Gradient)](https://arxiv.org/abs/1509.02971)       | Lillicrap et al   | 2015 |
| [TD3 (Twin Delayed DDPG)](https://arxiv.org/abs/1802.09477)                         | Fujimoto et al    | 2018 |
| [SAC (Soft Actor-Critic)](https://arxiv.org/abs/1801.01290)                         | Haarnoja et al    | 2018 |
| [DQN (Deep Q-Networks)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)                           | Mnih et al        | 2013 |
| [C51 (Categorical 51-Atom DQN)](https://arxiv.org/abs/1707.06887)                   | Bellemare et al   | 2017 |
| [QR-DQN (Quantile Regression DQN)](https://arxiv.org/abs/1710.10044)               | Dabney et al      | 2017 |
| [HER (Hindsight Experience Replay)](https://arxiv.org/abs/1707.01495)               | Andrychowicz et al| 2017 |
| [World Models](https://worldmodels.github.io/)                                  | Ha and Schmidhuber| 2018 |
| [I2A (Imagination-Augmented Agents)](https://arxiv.org/abs/1707.06203)              | Weber et al       | 2017 |
| [MBMF (Model-Based RL with Model-Free Fine-Tuning)](https://sites.google.com/view/mbmf)| Nagabandi et al  | 2017 |
| [MBVE (Model-Based Value Expansion)](https://arxiv.org/abs/1803.00101)              | Feinberg et al    | 2018 |
| [AlphaZero](https://arxiv.org/abs/1712.01815)                                      | Silver et al      | 2017 |
| [A Path Towards Autonomous Machine Intelligence](https://openreview.net/pdf?id=BZ5a1r-kVsf)|LeCun et al| 2022 | 
| [DreamerV3](https://arxiv.org/pdf/2301.04104.pdf)                                  | Hafner et al      | 2023|

  3. Model & Algorithm
      * DP
      * Monte Carlo Methods
      * TD
      * DQN
      * REINFORCE
      * PPO
      * DDPG
      
  4. Task
      * Navigation
      * Continous Control
      * 
  5. Case Study






## Main task

- Mathmatic
- Data Processing
- Machine Learning 
- Deep Learning
- Reinforcement Learning


## Reading



## Contributing

Guidelines for contributing to the project, including instructions for setting up a development environment and submitting pull requests.

## License

Information about the project's license and any relevant disclaimers.
